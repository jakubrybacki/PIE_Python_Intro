{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Wczytanie danych\n",
        "W trakcie zajęć będziemy pracować nad podstawowym zbiorem danych dotyczących teksów z platformy [Kaggle](https://www.kaggle.com/competitions/nlp-getting-started/overview)\n",
        "\n",
        "W pierwszej kolejności wczytajmy dane za pomocą pakietu *pandas*. Interesuje nas plik *train.csv*\n",
        "\n",
        "Organizujemy nazwę ścieżki pliku:"
      ],
      "metadata": {
        "id": "-VDNKhSrDEVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "filePath = os.getcwd() + \"/train.csv\"\n",
        "filePath"
      ],
      "metadata": {
        "id": "-KhHCHq9DUsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "W tym bloku wczytujemy dane:"
      ],
      "metadata": {
        "id": "bHuxhTAZDk62"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lpu-O2QlC651"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(filePath)\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocesowanie tekstu\n",
        "W pierwszej kolejności przygotowujemy tekst do pracy, tak jak robiliśmy to na poprzednich zajęciach - w pierwszej kolejnosci usuńmy cyfry, wielkie litery etc."
      ],
      "metadata": {
        "id": "MxD-LVG5EMZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "\n",
        "def linkRemoval(text):\n",
        "    text = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', \" \", text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    return text\n",
        "\n",
        "df['tokenized_texts'] = df['text'].str.lower()\n",
        "df['tokenized_texts'] = df['tokenized_texts'].apply(linkRemoval)\n",
        "\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "tT2sol99IjZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Następnie rozdzielamy na wyrazy:"
      ],
      "metadata": {
        "id": "8O8lSeHYKBal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
        "df['tokenized_list'] = df['tokenized_texts'].apply(tokenizer.tokenize)\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "055j9DsqJ5Wt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I usuwamy puste słowa:"
      ],
      "metadata": {
        "id": "kswtA_glLQ2_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stop = nltk.corpus.stopwords.words('english')\n",
        "df['tokenized_list'] = df['tokenized_list'].apply(lambda x: [word for word in x if word not in (stop)])\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "pohLubaSLRKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Dokonujemy stemmingu i lematyzacji:"
      ],
      "metadata": {
        "id": "k_Zw0lHrLRuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "\n",
        "def stemmer_text(word_list):\n",
        "    stemmer = nltk.stem.porter.PorterStemmer()\n",
        "    return [stemmer.stem(w) for w in word_list]\n",
        "\n",
        "def lemmatize_text(word_list):\n",
        "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "    return [lemmatizer.lemmatize(w) for w in word_list]\n",
        "\n",
        "df['tokenized_list'] = df['tokenized_list'].apply(stemmer_text)\n",
        "df['tokenized_list'] = df['tokenized_list'].apply(lemmatize_text)\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "eMrTNqNALSCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['final_string'] = df['tokenized_list'].apply(lambda x: ' '.join([str(i) for i in x]))"
      ],
      "metadata": {
        "id": "p3isYQctNxzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chmura słów\n",
        "Dla zwizualizowania sobie danych stworzymy prostą chmurę słów."
      ],
      "metadata": {
        "id": "sOekWn9LLVN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def printWordcloud(df_column):\n",
        "    from wordcloud import WordCloud\n",
        "    import matplotlib.pyplot as plt\n",
        "    list_Comments = df_column.to_list()\n",
        "    string_comments = ' '.join(list_Comments)\n",
        "\n",
        "    wordcloud = WordCloud(background_color=\"white\",\n",
        "                          width=800, height=400).generate(string_comments)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "printWordcloud(df.final_string)"
      ],
      "metadata": {
        "id": "7Ksh6Jf0M9XP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelowanie tematów - algorytm BERT\n",
        "Istnieje kilka algorytmów, które pozwalają określić podobieństwo różnych wypowiedzi - teraz zobaczymy algorytm bazujący na modelu językowy Google - BERT.\n",
        "\n",
        "Paczka BERTopic bazować będzie na:\n",
        "1.   Redukcji wymiaru danych - [UMAP](https://umap-learn.readthedocs.io/en/latest/index.html)\n",
        "2.   Wydzielanie klastrów - [HDBScan](https://hdbscan.readthedocs.io/en/latest/index.html)\n",
        "3.   Sieciach neuronowych - tutaj będą to implementacje bibliotek [Keras](https://keras.io/) i [Tensorflow](https://www.tensorflow.org/)\n",
        "\n"
      ],
      "metadata": {
        "id": "lWPyZPjPPuH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install bertopic"
      ],
      "metadata": {
        "id": "Tg7haTCyOxWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pierwszym krokiem jest dopasowanie sieci neuronowej do danych - warstwy zawierają poprzednio wskazane algorytmy:"
      ],
      "metadata": {
        "id": "qVKdj43N8quA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bertopic import BERTopic\n",
        "model = BERTopic()\n",
        "topics, probs = model.fit_transform(df.final_string.tolist())"
      ],
      "metadata": {
        "id": "Xyh_2x71POQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Możemy przypisać tematy do naszych tweetów:"
      ],
      "metadata": {
        "id": "OMhxgnaPAfPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['topic'] = topics\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "6lT50K90_KPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "W pierwszej kolejności możemy zobaczyć jak algorytm dobrał teksty w wątki.\n",
        "1.   Pod liczbą -1 ukrywają się posty jednoznacznie niesklasyfikowane jako powtarzalne wzroce.\n",
        "2.   Każde kolejne grupy zawierać będa słowa kluczowe oraz prawdopodobieństwo pojawienia się słowa w tekście.\n"
      ],
      "metadata": {
        "id": "c4LCNDci-bRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_results =  model.get_topic_info()\n",
        "df_results.head(10)\n",
        "\n",
        "#Same częstotliwości\n",
        "#model.get_topic_freq()"
      ],
      "metadata": {
        "id": "608g-kD-PaZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sieć neuronowa określa również prawdopodobieństwo pojawienia się danego słowa w tekście o określonym temacie:"
      ],
      "metadata": {
        "id": "bgX259UxBNTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.get_topics()\n",
        "model.get_topic(0)"
      ],
      "metadata": {
        "id": "eRsJYl3rPXRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mamy też trochę szerszy interfejs do przedstawiania dokumentów:"
      ],
      "metadata": {
        "id": "yP2meakJA9aC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_classification =  model.get_document_info(df.final_string)\n",
        "df_classification.head(10)"
      ],
      "metadata": {
        "id": "F6reQXf2RBZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kilka gotowych wykresów:"
      ],
      "metadata": {
        "id": "U5ucN_9SBoMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.visualize_topics()"
      ],
      "metadata": {
        "id": "PyCYK6pR9yK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.visualize_barchart()"
      ],
      "metadata": {
        "id": "BJFezB2WRK3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.visualize_heatmap()"
      ],
      "metadata": {
        "id": "hydosjF7-FOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Więcej informacji ma [Github.io twórcy](https://maartengr.github.io/BERTopic/index.*html*).\n",
        "\n",
        "Szybkie ograniczenie ramki do tekstów z danego tematu dostępne w pakiecie Pandas:"
      ],
      "metadata": {
        "id": "igPQ_GSuBrnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.query(\"topic == 2\")"
      ],
      "metadata": {
        "id": "ShCIcDciTFK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analiza Sentymentu - Algorytm VADER\n",
        "Kolejnym częstym zastosowaniem pojawiającym się w analizach tekstów jest określenie czy są one pozytywne czy negatywne. Bardziej złożone algorytmy potrafią wskazać też emocje takie jak strach, złość czy radość.\n",
        "\n",
        "Przejrzymy najprostrzy algorytm VADER - zobaczmy jego funkcjonowanie dla pierwszego tekstu. Trzy linijki kodu poniżej generują obiekt, który buduje analizy sentymentu."
      ],
      "metadata": {
        "id": "pqK5v_wcLuLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment import SentimentAnalyzer\n",
        "sia = nltk.sentiment.SentimentIntensityAnalyzer()"
      ],
      "metadata": {
        "id": "vNrc_rklOO9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To będzie nasz tekst na którym pracujemy:"
      ],
      "metadata": {
        "id": "3NcI0oqNPlbB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tekst = df.text.iloc[1]\n",
        "tekst"
      ],
      "metadata": {
        "id": "p083F8tiOWkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Co generuje algorytm?\n",
        "\n",
        "Mamy słownik, tudzież ramkę JSON. Pokazuje procentowo jaka część zdania ma słowa pozytywne, negatywne i neutralne. Na koniec indeks zbiorczy."
      ],
      "metadata": {
        "id": "9efm5vKKPpKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sia.polarity_scores(tekst)"
      ],
      "metadata": {
        "id": "o11RfvMKPsxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zaaplikujmy to do całej ramki danych. Wygenerujemy funkcję, którą przekażemy do każdego tekstu w ramce."
      ],
      "metadata": {
        "id": "LGpRDp4KQWXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sentiment(word_list):\n",
        "    sia = nltk.sentiment.SentimentIntensityAnalyzer()\n",
        "    tempString = ' '.join(word_list)\n",
        "    return sia.polarity_scores(tempString)\n",
        "\n",
        "df['sentiment'] = df['tokenized_list'].apply(sentiment)\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "kYluJ8mFQayu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Możemy rozłożyć ramkę na kolejne kolumny - robi się to tak:"
      ],
      "metadata": {
        "id": "iKAC7FfoToWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = pd.json_normalize(df.sentiment)\n",
        "df = pd.concat([df, df2], axis=1)"
      ],
      "metadata": {
        "id": "iTrCVtsZTi5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stwórzmy własny indeks dyfuzyjny:"
      ],
      "metadata": {
        "id": "usPLB_ImTvgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"VADER_index\"] = (df.pos - df.neg)/(df.pos + df.neg)\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "z7Hrx5KLTwEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ngramy i tagowanie częsci zdania\n",
        "Na koniec jeszcze dwa zastosowania:\n",
        "1.   Możemy szukać określonych zbitków słów następujących po sobie - to są tzw. N-gramy. Dwa słowa to Bigram, Trzy słowa trigram etc.\n",
        "2.   Możemy też otagować każde ze słów - czy to czasownik, przymiotnik etc."
      ],
      "metadata": {
        "id": "pqGkKMGqUsF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "speech_list = df['final_string'].to_list()\n",
        "speech_string = ' '.join(speech_list)\n",
        "words = speech_string.split()"
      ],
      "metadata": {
        "id": "a4ed_rFVPivK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 3\n",
        "ngrams_obj = nltk.ngrams(words, n)\n",
        "\n",
        "fdist = nltk.FreqDist(ngrams_obj)"
      ],
      "metadata": {
        "id": "IzRF_guEVgYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stosunkowo prosto możemy utrworzyć sobie ramkę z najczęściej powtarzającymi się słowami:"
      ],
      "metadata": {
        "id": "F6ojOOjiPT4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frame = pd.DataFrame(fdist.items() , columns=['Words','Frequency'])\n",
        "frame = frame[frame['Frequency'] >= 10]\n",
        "frame.sort_values('Frequency', ascending=False).head(10)"
      ],
      "metadata": {
        "id": "c4a-bGkjPTQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tagowanie jest stosunkowo proste:"
      ],
      "metadata": {
        "id": "RWAj44FpVf7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "def tagowanie(tekst):\n",
        "  tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
        "  tokenized_list = tokenizer.tokenize(tekst)\n",
        "  return nltk.pos_tag(tokenized_list)\n",
        "\n",
        "df[\"POS\"] = df.text.apply(tagowanie)"
      ],
      "metadata": {
        "id": "vFrDMpyUVgwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Przyjrzymy się wynikom - lista tego co oznaczają kolejne tagi dostępna jest [tutaj](https://www.learntek.org/blog/categorizing-pos-tagging-nltk-python/)."
      ],
      "metadata": {
        "id": "b2iNDP7_W4GB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[[\"text\",\"POS\"]].head(10)"
      ],
      "metadata": {
        "id": "YIH_MxZTWUbm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}